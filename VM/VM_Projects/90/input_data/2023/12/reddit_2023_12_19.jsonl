{"title": "AMD is disrupting Nvidia", "selftext": "[removed]", "id": "18lzlcy", "created_utc": 1702984798.0, "score": 174, "upvote_ratio": 0.77, "num_comments": 93, "comments": [{"body": "You mention Innovators Dillemma, but are you also aware of the job to be done? (*Competing Against Luck*)\n\nAMD absolutely has potential to disrupt, but they have been missing a critical job to be done, which is why they were not the ones to own the AI moment as it arose.\n\nPerhaps the crowd sourcing will pay off in the end, but I think that is one level too low on the stack for platforming - more people are interested in AI application development than AI stack development.\n\nI own both, but not at equal weight. I think AMD and NVDA offerings naturally segment their markets - people buying into each know what they're getting (and what they're not).\n\nGood post. It is these lines of thinking that can give you an edge.", "score": 136, "replies": [{"body": "Curious what is the JBTD that AMD is lacking? Iâ€™m asking as some genuinely not as informed about Semicon having last invested in both these companies in 2017 (and sold in 2019â€¦so I didnâ€™t get far).", "score": 14, "replies": [{"body": "Prefacing with determing the JTBD is non-trivial, and that it may be an evolving target.\n\nWhat progress do their customers want to make in their lives/work?\n\nOk - these neural nets and other ML things are very powerful, but how can I use them?\n\nHave you ever paid more for a product than the cheapest alternative? Why?\n\n\nIMO, it looks like NVDA troubled themselves with talking to geneticists/enzymologists/designers/developers to understand what it was they were trying to do, and then took that home to HQ to make sure that those people/use cases were satisfied - not with the compute, but with the complete experience. For non-hyper-scalers, compute per dollar is much lower on the list than \"how much will this multiply my productivity/abilities/output\".  To satisfy this is not free in either the mental or fiscal sense, so it required deliberate intent in the company to pursue.\n\nOTOH, it looks like AMD focused on absolute margin and compute efficiency, with particular experiences by the wayside. Because if you need cost efficiency, and you design hardware, why would you waste margin on any more software development than you need? AMD never cared to be first for a technology like DLSS/FSR, they cared to make hardware that could kind of do it for the lowest up-front cost possible.\n\n\nSo, fundamentally, I think these companies are pursuing nearly completely different goals. While some of the products might look similar (androids and iPhones are pretty similar looking on the surface), the internal processes and dogmas followed to get there are drastically different, and results in a stark divide between the customers they attract (most Android people would never want to go to iPhone, and vice versa).\n\n\n\nBut - this field is so complicated, and while I've made a lot of efforts to know and understand as much as I can, there is so much more. Also, my covering of the JTBD here is not great and would encourage anyone interested in knowing more to watch any Clayton Christensen talks, and consider the books Innovators Dillemma and Competing Against Luck. Very empowering perspectives.", "score": 28, "replies": []}]}, {"body": "a good tip for those who are tracking AI chip markets. Check if manufacturer GPUs are available and being used on top three clouds ie aws, azure, gcp. A simple lookup will find you nvidia absolutely dominates the GPU vms and usage. Unless someone disrupts that, it's all speculations. \n\nI am personally interested in Amazon, Microsoft and Google's hardware. Cutting a middle man has huge scope of disruption even if they are 80% of best offerings.", "score": 9, "replies": [{"body": "Yes, your last idea is one I have been focusing on. What I figure is, with the trajectories of everything, amzn msft and goog would be irresponsible to not be developing their own. \n\nBut will it be a replacement, or an aside? Can they really just up and decide to replace NVDA completely, and easily? I put weight on the idea that it can't be that easy, and that the work NVDA has done to get where they are was necessary. But I don't know enough to know that with confidence.\n\n\nWhich is part of the reason I own shares in all of those companies. The pie is plenty big, and growing fast - they can all win without needing to kill others.", "score": 8, "replies": [{"body": "Google has already their own chips for ML applications (TPUs)", "score": 4, "replies": [{"body": "Mhmm, and they are quite good at what they do. \n\n\nBut man does google suck at making and supporting products. They would have to make quite the presentation to sway anyone in the NVDA ecosystem to buy in. Likely best used in their own cloud, and they can give access via services to ease use (but again, they suck so much at this kind of thing. So much potential, such horrific followthroughs.)", "score": 3, "replies": [{"body": "Does Google have any plan to commercialize the TPUs? If so, totally agree with what you're saying given their history.\n\nPerhaps the point is just to be more vertically integrated and reap the associated benefits, and the spillover effect is NVDA losing them as a customer, at least somewhat.", "score": 1, "replies": []}]}]}]}]}, {"body": "One trades at 10.2 times sales and the other trades at 20 times sales.\n\nJustified?\n\nI've read some highly varying opinions at r/StockDeepDives.", "score": 1, "replies": [{"body": "That's the trillion dollar question, isn't it? It could be justified, until news come out tomorrow that completely changes the answer. The longer the market maintains the difference, the more the difference becomes real/justified/ingrained.\n\n\nPersonally, I enjoy borrowing the Anthropic Principle from cosmologists to explain why things are the way they are, and to question if they should be/need to be/must be.\n\n\nIn that lense: why was chatGPT trained on NVDA cards and not AMD? Why can NVDA demand such premiums on their products? Why is NVDA always the first to push new graphics technology, while AMD always plays second fiddle (eg. DLSS vs FSR)?\n\nTo me, it can all be explained as: if AMD was going to win AI, they would be winning. We observe the ChatGPT breakthrough, in November 2022 on NVDA cards, because it was the only way it was going to happen in November 2022 - the availability and capability of compute in the years leading up to it determined their hardware choice, and the capability and availability of that compute was determined by the compute years before and compounded by individual firms choices as dictated by leadership, all leading up to the fact that it happened as it did.\n\nWhy would it suddenly change to \"Theres no way NVDA can maintain\", when it has been over a decade of maintaining? If anything, the ChatGPT moment has compounded their prospects as they now have even more resources to continue their progress.\n\n\nA company is never too large to have their legs swept, that's for sure. But in a weird twisted way, it feels like many have set this as the base case expectation, when it really would be the exception. NVDA is not sleeping at the wheel.\n\n\nETA: Here's a fun thought experiment that works with that anthropic-principle-like lense: why did China not end up overtaking USA as the defacto superpower and market? There was a lot of speculation in the early 2000's that it was coming/inevitable/etc. They also technically had all the resources and political capital to do pretty much anything.\n\nBut they didn't. I would say it has something to do with the inherent differences between the two countries, with personal liberties perhaps being a top factor, and how those differences propagate through everything and everyone they interact with. \n\n\nThings are very complex, and why something happens even more so. But at the end of the day, things happen for reasons, and the reasons will often be intrinsic. Extrinsic is easy to spot anyways.\n\n\nETA2: Another way of saying the same thing, this time borrowing from the mathematicians: a company's success not a state function, it is a path function. Looking at measurements at a point in time barely tells half the story - much more important was the path they traveled to get there. I can tell you the car is going 50 mph, but that tells you nothing about whether the next reading will be 49 or 51.", "score": 10, "replies": [{"body": "Don't forget the possibilities of a merger to advance their dominance.  Say Qualcomm or another wireless chip maker.  Where they will be able to expand.", "score": 2, "replies": []}, {"body": "I like how you write.", "score": 2, "replies": []}]}]}]}, {"body": "Went to AWS reinvent this year where the subject was gen AI. All the talks were about Nvidia's partnership with AWS and how AWS is planning to build a mega  AI computer with Nvidia GPUs. A panel of AI experts also talked about the scarcity of resources right now and how if your a start up you should just use lower tier nvidia chips to do AI modeling. Because nvidia resources are too scarce right now.", "score": 42, "replies": [{"body": "AWS is a big AMD customer. As soon as AMD releases the new chip, I will bet the farm  that AWS will offer that. That is what happen with EPYC EC2", "score": 8, "replies": [{"body": "Yea AWS supplies all chips but any customer that can will use graviton and nvidia chips for their work. Graviton has way better price to performance than intel and AMD so only the unlucky few choose those instances", "score": 4, "replies": []}]}]}, {"body": "Pytorch has â€œsupportedâ€ AMD gpus for a couple years now. Why hasnâ€™t AMD taken over yet?", "score": 52, "replies": [{"body": "Hasn't AMD only just launched their AI chip like last week?", "score": 4, "replies": [{"body": "AMD has had a cuda alternative for a while now, but their software/hardware combination is a complete mess for doing deep learning. Itâ€™s just one error after another.\n\nAI chips are just GPUs that are specialized to make training faster, however, if their top end consumer gpu canâ€™t do (slower) training without problems, they will also have issues with their AI chipsâ€¦", "score": 28, "replies": [{"body": "Baring some sort of innovation.", "score": 1, "replies": []}]}]}]}, {"body": "lol nope.\n\nDeep Learning researcher here. Officially AMD has supported PyTorch (and Tensorflow to an extend) for a while now, but getting it to run models properly (both training and inference) is just one giant headache - itâ€™s one error after anotherâ€¦\n\nNo one is gonna ditch Nvidia for AMD for AI any time soon. Ecosystem takes ages to develop.", "score": 74, "replies": [{"body": "Researcher here as well 100% agreed.", "score": 20, "replies": []}, {"body": "> itâ€™s one error after anotherâ€¦\n\nHow recent is this experience? This might have been the case a year or two ago, but I feel like it keeps getting better and better every ROCm update, and AMD seem to take this pretty seriously. Most of my LLM stuff just works.", "score": 5, "replies": [{"body": "does it ? I just installed (and still use) it every day (stable diffusion rocm support) and it... sucks ? my card is much better at basic specs compared to \"comparable\" nvdia cards, yet the stablediffusin performance is abysmal. \n\nthats on windows. wasnÂ´t necessarily complicated to setup, still more hassle than the same stuff with an nvidia card.\n\nand to the \"just works\"... i canÂ´t train, it doesnÂ´t support basic functionality thats available with cuda and nvidia cards.\n\nso...yeah. no", "score": 12, "replies": [{"body": "In this stock subreddit, what really matters is the experience with the datacenter (or workstation) cards on enterprise systems, when discussing future sales of MI-series cards. That is what I operate. I understand your frustration with presumably consumer level cards on Windows, but that is not really a proper relevant target yet, the support for those just came out an update or two ago.", "score": -1, "replies": [{"body": "Datacenter and consumer cards are made with the same underlying technology.", "score": 4, "replies": []}, {"body": "My most recent experience with ROCm and AMD is a few months old. I donâ€™t doubt they would try to improve things, but highly doubt itâ€™s smoothly running already, given the amount and type of errors I had. \n\nAt this point, although Nvidia is expensive, the difference in price isnâ€™t big enough for this to worth the headache. This is especially the case for companies, where manpower is much more expensive than the price difference in GPU prices.\n\nThe top end consumer card vs data center card experience being different is a weak argument. They all use the same software. Hardware is mostly the same, except the AI chips have much larger gpu memories and many more tensor and cuda cores.", "score": 1, "replies": []}]}]}]}, {"body": "[deleted]", "score": 0, "replies": [{"body": "Yes, both their stock price and GPU are expensive. So some competition would be nice for consumers, but for now, there is no real alternativeâ€¦", "score": 0, "replies": []}]}]}, {"body": ">This is because when you make a chip out of chiplets, if one of the components goes wrong you donÂ´t have to throw the entire thing away. Hence, yields are much higher than in monolithic architectures.\n\nPatently untrue. It is standard practice in the chip world to laser off sections of the chip that are unusable/not up to specification to produce lower tier products. It's called chip binning.\n\nA lot of the consumer hardware sharing chips with the enterprise hardware is often a binned version of the enterprise chip.\n\nYou could make the argument that Nvidia doesn't sell its H100/H200 (among others) silicon at the consumer level, and you would be correct. However, consider Nvidia's gross and net margins - in spite of wastage, the company is still raking in cash hand over fist and will continue to do so for as long as CUDA is viable.\n\nThe questions to ask then are:\n\n1. Just how good/bad are Nvidia's yields?\n2. How much money is Nvidia losing on these bad chips? (clearly not much, given their huge margins)\n3. What kind of hurdles can we expect in newer process nodes? How will they drag down Nvidia's balance sheets?\n\nYou say that the industry has moved to pytorch. However you seem to have made a huge misinterpretation on what Pytorch is and what CUDA is. CUDA is not an AI framework. It is an API used to program shader processors to work Nvidia's graphics hardware. Instead of programming graphics shaders, CUDA is used to program general purpose compute tasks in a highly parallel manner. This language can be used literally on any Nvidia chip starting with the G8x series of chips released all the way back in 2007-2008, all the way to today. So the backend are guaranteed to be mature and mostly trouble-free, and developers with sufficient experience should be easier to find. At that point, if you need CUDA you have no choice but to buy Nvidia chips.\n\nBy comparison, Pytorch is a higher level framework that sits on top of CUDA and other lower level APIs. So for the customer deciding between AMD and Nvidia chips the following considerations will be made:\n\n1. Is it more efficient to roll our own AI technology or to use Pytorch or some other library? Most of the times, Pytorch or its peers will make a strong case for themselves.\n2. Now that we know what we want to use, do we buy Nvidia or AMD hardware? Over here the customer, depending on their position in the value chain, will consider performance, price and space. The order of priority will depend on their needs.\n * For example, a company making huge datacentres will want to fit the highest performance in the least amount of space at the lowest cost possible.\n * A company renting hardware from one of these datacentres to develop AI will only care about the performance:cost ratio, because they can just rent more capacity as needed.\n\nCompanies that run AI-assisted services but don't do any in-house AI development will likely go with the cheapest options that service their needs. In this space I believe AMD and Nvidia may both be at a disadvantage because Google already has excellent AI inference chips and are renting them out to clients via their cloud services. Amazon and Microsoft have also made significant investments in this sphere and have deep pockets so they cannot be ruled out in the long term.\n\nHowever, in spite of all this, Nvidia still will lead because it will take a lot of work to actually produce products that can comprehensively beat Nvidia at their price points.\n\nAnother thing people seem to forget is the performance bottlenecks that chiplets introduce. When you have multiple compute chips tied together using a chiplet design, you necessarily need to wire them to each other either via a PCB or via another chip (as is the case with AMD's Infinity Fabric). This can actually cause noticeable bottlenecks in inter-chip communication. Given that GPUs are by design highly parallel and AI LLMs necessarily take a lot of parallel processing to be trained efficiently, the chiplet design may or may not be the best way forward.", "score": 10, "replies": [{"body": "Check OP's history and this obvious superficial pump posting becomes obvious.", "score": 2, "replies": []}]}, {"body": "AMD hasn't taken marketshare from Nvidia in Gaming GPUs, I wouldn't hold my breath for them to take it in Datacenters where it's more difficult.", "score": 6, "replies": []}, {"body": "AMD is riskier than NVDA.\n\nIf you look at their non-GAAP profits, $1.1bn in the recent quarter which would be 4.5bn annual, against their 229bn market cap, they are priced for high growth.\n\nNVDA's market cap of 1216bn versus their 10bn qtrly profit or 40bn annual, means they are trading much lower than AMD.\n\nIt's better to hold the incumbent unless the challenger is already taking share.", "score": 5, "replies": []}, {"body": "Pytorch and Cuda are 2 completely different players in the AI stack. Devs use the pytorch framework to run code using cuda libraries. Apples and oranges both fruits.", "score": 6, "replies": []}, {"body": "Iâ€™ve had Nvidia shares since 2018, and been following the company and trends since then. Iâ€™ve heard about AMD taking over Nvidia every single year. But it never happens.", "score": 4, "replies": []}, {"body": "Nvidia can go chiplet any time they want to. This isn't something unique to AMD, you know. Thus far, they haven't needed to because they are already leading in every conceivable way. Even with lower yields on their monolithic dies, Nvidia's margins are still *significantly* higher than AMD's. We still dont even know if the MI300X is *actually* better than the H100, much less the H200. Not to mention, Blackwell is around the corner and is supposed to be a 4-6x improvement over the H100. So, if AMD doesn't have any cards up their sleeves, I don't see any of this happening. Though it is impressive that they can even compete with Nvidia at all. I wish they put that much effort into their dGPU division.\n\nIn order for them to truly disrupt Nvidia, they'll have to bring out a convincingly superior product AND for CUDA to fade away into irrelevance. Not marketing about what's in the future, but actually ON the market and readily available. They can claim whatever performance numbers they like, but until that becomes reality, I just don't see how they make any meaningful impact on Nvidia's dominance. Time will tell, though.\n\nAs for the stock angle, AMD is actually *much* more overpriced than Nvidia is. Current valuations are 1092 times current earnings for crying out loud. Even in the coming months where the data center revenue picks up, it's still sitting at 52.40 times forward earnings, which is one of the most bloated stocks on the market. So unless their revenue beats analysts' predictions and their own guidance by at least 30%, Nvidia is still a better buy at 40.74 times forward earnings.", "score": 16, "replies": [{"body": "https://www.extremetech.com/computing/nvidia-is-allegedly-planning-to-use-chiplets-for-next-gen-blackwell-gpus", "score": 2, "replies": []}, {"body": "Where do you get that amd is sitting at 52.40 times forward earnings and nvidia at 40.74? Im Interested.", "score": 1, "replies": [{"body": "[deleted]", "score": 10, "replies": [{"body": "> Despite losing all of its China's revenue due to the chip export ban\n\nOfficially yes, though they have been finding ways around that, at least until recently when the SEC said they were losing their patience with this sort of thing.", "score": 1, "replies": [{"body": "[deleted]", "score": 1, "replies": [{"body": "Guess they haven't lost all of their China revenue.", "score": 0, "replies": []}]}]}]}, {"body": "Schwab app is what I use.", "score": 1, "replies": []}]}, {"body": "OP history is instructive.  They spam these sensationalized pumps, when they're not bragging of weird magnet treatments for covid.", "score": 1, "replies": [{"body": "I hadn't thought about that before, so I looked at his profile. First thing that pops up is \"early AMD investor with xxxx profits\". Yep, you were right. Just another shameless shilling post.", "score": 2, "replies": []}]}]}, {"body": "ok buddy, FYI top sp500 companies have contracts with Nvidia to buy their gpus in massive bulks. The capital expenditures have already been made.", "score": 13, "replies": []}, {"body": "The winners will be the ones that build something useful in these chips. Pick your winners. I will bet on Google.", "score": 3, "replies": []}, {"body": "out of the box ? try installing python / pytorch stuff that relies on it (o example being stable diffusion) and tell me how great that runs on amd compared to nvidia. yes it does, but just not as good. \n\nso no. amd - especially in the AI segment - still has some serious work to do. given their track record I do not believe in any major breakthrough in the next coming 6 months. \n\nsad part really is thats not even their hardware, but their drivers and softare is still bad. \n\nin games, their gpu Â´s are oftentimes better and have far superior performnce per dollar value. something that is important when it comes to scaling: bang for buck. but still, nvidia is the goto solutions because it just is easier to get to work", "score": 3, "replies": []}, {"body": "Until AMD gets serious about DL software supporting their hardware, it isn't going to overtake NVIDIA. People have been talking about AMD cards competing with NVIDIA cards for deep learning since 2018, it still hasn't happened. Their best bet is to go all in on OpenAIs Triton, which works with either AMD or NVIDIA cards, as a CUDA replacement,", "score": 3, "replies": []}, {"body": "ðŸ¤£", "score": 9, "replies": []}, {"body": "No", "score": 4, "replies": []}, {"body": "Chiplets interconnection is what expensive these days", "score": 2, "replies": []}, {"body": "Just buy both, I own both. Up huge in both. Don't understand why that's complicated.", "score": 2, "replies": [{"body": "Because people put money where their heart is", "score": 1, "replies": []}]}, {"body": "Plenty of room for all to play. Nvda, if government stops messing with them, could be the biggest company in the world in the next 5 years.", "score": 3, "replies": []}, {"body": "why do chiplets have lower margins but the yield of them overall are higher?\n\nwouldn't the higher yield equate to them having higher margins? if not, the lower margins issue must be **really bad** in comparison", "score": 2, "replies": []}, {"body": "So buy AMD calls, got it.", "score": 2, "replies": []}, {"body": "Lol no.\n\nMi300x costs over 2x to make vs H100.\n\nAmd is selling mi300x near loss. \n\nNvidia has 70%+ margin on each h100 sold. \n\nAmd uses chiplets cause they hit a peformance improvement wall on mon chips.\n\nNvidia still did not hit that.", "score": 2, "replies": []}, {"body": "I know p/e isn't a precise measure, but over 1000 seems a tad high", "score": 1, "replies": [{"body": "thatâ€™s due to xlnx acquisition.  Forward PE 30ish.  Still high, but thatâ€™s the number.", "score": 12, "replies": [{"body": "From the numbers I see, it's 52.40 times forward earnings, so it's VERY high. Significantly higher than any other technology stock that I know of.", "score": 3, "replies": [{"body": "They are expected to grow quickly from a much smaller base than NVDA", "score": 1, "replies": [{"body": "That's not what the forward PE says. 52.40 forward PE means it's extremely overpriced at the expected revenue levels by industry analysts. Unless it beats those estimates and their own guidance by 30%, Nvidia is still the better buy, larger market cap or not. Everything else is just speculation. \n\nNvidia earnings per share: $7.57\nAMD earnings per share: $0.12\n\nWanna know what AMD's estimated *forward* earnings per share is? $0.62. Yeah, not even 1/10 of Nvidia's, and this is including all the huge gains in commercial production. There is so much speculation in AMD's stock that it borders on insanity.", "score": 1, "replies": [{"body": "You are making the cardinal sin of assuming investors only value the next year earnings. The stock price reflects future growth expectations further out than 1 year(ie forward p/e). Also, when you focus on p/e for a growth company you often miss the forest for the trees. In addition you are getting the impacts of the xylinx acquisition affecting gaap p/e making that metric pretty flawed for AMD. I use P/e a lot and if you focus purely on that you will be investing in some dinosaurs and miss where the money is and has been.", "score": -1, "replies": [{"body": "Is that so? Do you really think at ANY point in the future that AMD will grow enough to justify being valued 30% higher than where Nvidia sits right now? A 224 billion dollar market cap company being valued 30% higher price to forward earnings than Nvidia at 1.2 trillion? Their estimated EPS for the next year is just $0.67 per share ffs, and that's after current profits of just $0.12 per share ðŸ˜†. \n\nNvidia is absolutely dominant across all their product segments, so its literally impossible for AMD to reach those valuations unless Nvidia goes belly up. Playing devils advocate, even if AMD traded places with Nvidia, it's still impossible for them to grow 30% beyond that because their profit margins are so much worse than Nvidia's. Their data center offerings are being sold at a loss just to gain market position, where Nvidia is selling theirs at record profits. AMD doesn't even hold a majority in their best market segment, and in their worst segment its embarrassingly bad at less than 10% market share. Not to mention Nvidia spends 60% more on R&D than AMD does, so the likelihood AMD ever overtakes Nvidia is the longest of long shots.\n\nIf you wanna keep buying AMD, be my guest. They're barely profitable at this point, and there's countless other companies that not only have better earnings today, but also higher projected growth in the future. But you do you, boo. Me? I'll stick to companies who's financials reflect reality.", "score": 1, "replies": []}]}]}]}]}]}]}, {"body": "Honestly.. Amd will and always will be considered 2â€™nd and even 3â€™rd. Thereâ€™s GENERIC (think Teva) and then thereâ€™s Name brand.", "score": -8, "replies": [{"body": "You could have said that about them and intel 10 years ago and youâ€™d be dead wrong", "score": 11, "replies": []}, {"body": "Never say never!", "score": 1, "replies": []}, {"body": "OP did not say or imply that. \"Taking marketshare\" is not the same as \"overtaking marketshare\".  The point of this discussion is AMD's future stock value.", "score": 0, "replies": []}, {"body": "Yeah itâ€™s dumbest comment in a thread right there dot dot Jensen himself has said AMD is coming for their lunch", "score": -1, "replies": [{"body": "When did he say that?", "score": 1, "replies": []}]}]}, {"body": "[AMD](https://amassinginvestment.com/symbol/AMD) recently launched itself at Nvidia's AI supremacy, showing itself ready to disrupt Nvidia's long-standing dominance with a formidable new AI chip, the MI300Xâ€”an avant-garde GPU designed for the cutting edge of AI.\n\nNvidia currently owns an 80% market share in AI chips, so AMDâ€™s war drums set the stage for an exhilarating shake-up in the race to rule the world of AI accelerators.\n\nIf AMDâ€™s new GPUs are competitive, not only will the company benefit from increased data center sales, but also its ability to infuse each business segment with AI capabilities, along with improved margins.", "score": -4, "replies": [{"body": "Yep, and both XBOX and PS5 run on AMD. When the next revision cones out with all these breakrhroughs, it is going ti be awesome. The DC MI300 & MI300x for LLM training are the fastest growing sales amd has ever experienced", "score": 4, "replies": []}]}, {"body": "Frick yeah. Keep that RSI scary and this thing creeps up....", "score": 0, "replies": []}, {"body": "Interesting angle, thanks for sharing!\n\nFrom what I learned AMD is great at doing very efficient products at a reasonable price, given NVDA high margins it seems there is a lot AMD can eat there.", "score": 0, "replies": []}, {"body": "Also the 4090 and H800 GPUs/AI cards have been banned from sale to China. AMDs stuff can still be sold there. The AMD MI300 is likely to be banned but I donâ€™t believe it has been yet. China is a huge market for this stuff and their homegrown competitors are years (or more) away from having real products that matter.", "score": -1, "replies": [{"body": "No, there is a misconception that the US targeted Nvidia.\n\nAnything that has rtx 4090 AI performance has been banned, INCLUDING the AMD datacenter GPUs because they can at least keep up with the rtx 4090 in AI so banned", "score": 1, "replies": [{"body": "The consumer grade rx 7900 XT and rx 7900 XTX had a boost in sales after the 4090 was banned. The MI300 is AMDâ€™s AI chip and yea it is above the performance limit so is banned like I thought itâ€™d be. I thought it might not have made its way onto the list yet since it was just released", "score": 1, "replies": []}]}]}, {"body": "No buy AMD? 10 year hold?", "score": -2, "replies": []}, {"body": "I don't know if AMD will be able to beat Nvidia in raw capabilities, although I would expect them to do nicely regardless. Ultimately, I don't think AMD has to beat Nvidia for this to matter. All they need to do is offer an alternative.\n\nI have NOT looked at AMDs product offering, but I've been saying for a while on all the stupid Nvidia bull posts that they aren't as secure as people think. Even if they remain the leader, they will loose market share to competitors who can offer products that are good enough and at a lower price point.\n\nGiven how incredibly expensive Nvidia is (current PE, not forward PE, the latter being a fantasy number), it won't take much to push them down.", "score": 1, "replies": []}, {"body": "all I am hearing is now AMD cards will be just as overpriced as nvidia.\n\n&#x200B;\n\nLovely.", "score": 1, "replies": []}, {"body": "Just buy SOXL", "score": 1, "replies": []}, {"body": "I haven't seen so much wrong in a long time", "score": 1, "replies": []}, {"body": "RocM is the equivalent of CUDA as AMD is to Nvidia. Adoption of RocM has been about the same as AMD market share in the consumer market...\n\nThe only thing that *may* see AMD gain market share is the fact that RocM is open source. As a developer, we really appreciate open source.", "score": 1, "replies": []}, {"body": "I'm so happy with my chip etf. Rising tides and such.", "score": 1, "replies": []}, {"body": "I believe there is a third more important element at play here. Manufacturing, neither company has the ability to fab any of its own chips, both being tied to the whims of TSMC. Apple proved with its 3nm M series chips that they can pay there way to the front of the line and gain an edge on their competitors. Nvidia I believe is in a similar position, they have a much higher market cap, more cash on hand, and a faster development cycle. I think that even if the GPU compute playing field was leveled today AMD will always be at the mercy of TSMC and won't be able to get orders in fast enough or competitively enough to beat Nvidia.\n\nTo make matters worse Nvidia has a pretty decent lead in performance per watt on the GPU side which is really all that anyone serious about AI cares about. Finally AMD doesn't offer data center cards that can compete with anything Nvidia is doing right now, the war isn't occurring between a 7900xt and a 4090 it is between the H200 and.... nothing else.\n\nFor AMD to compete they would need to leap frog Nvidia and have N2 node designs ready late next year or early 2025 and an order size that Nvidia can't compete with. They would also need to pull off the same with Samsung to get memory to match. Then have an enterprise card that can compete with the H200 or better by 2026. Then also have a very well developed CUDA substitute also ready and in the wild because AI alone won't justify the massive investment AMD is going to need to make.\n\nMeanwhile I have a suspicion that Apple may enter the AI segment with a vengeance. M3 Ultra will likely come with 128-256gb of on chip memory with AI compute performance that bests either AMD or Nvidia at the workstation level. There is a chance I believe that Apple enters the server space again with the M4 series. They almost certainly would love the win of not relying on x86 chips for their own cloud services while the top tier M3 and future M4 won't sell enough in volume to keep margins high enough. For apple this is a win win. Higher margins, better yield, and a new predictable market to re-enter.", "score": 1, "replies": []}, {"body": "No one mentions and I guess no one understands \nXilinix . It's an entire market NVDA doesn't have . And they are dominators in their field. With rest good products", "score": 1, "replies": []}, {"body": "The problem with nvidia is that is has zero room for error. They have to exceed expectations every quarter for the next 100 years or the ghost of tanking price will come reality.", "score": 1, "replies": []}, {"body": "Itâ€™s been a while since I was invested in either, but there was a time when I made a lot of money owning NVDA *and* AMD", "score": 1, "replies": []}, {"body": "I think fpga is very exciting and like amd for their acquisition of xillinx. Itâ€™s such a pain to do any AI on a FPGA but I think itâ€™s the future of many (fast real time) applications. I also like ARM as microcontrollers are able to run simple pre trained ai models which is nuts to me (see tensor flow lite/ teensy 4.2), a direction I see gaining traction. Not an ai expert though just my $0.02", "score": 1, "replies": []}, {"body": "thereâ€™s a pile of cuda code out there that will never be migrated", "score": 1, "replies": []}, {"body": "Maybe over the short and medium term.  But I think over the long term you will see the chips more and more come from the companies offering the models.\n\nGoogle is already there.   They started on their TPUs over a decade ago and now on the fifth generation.\n\nThey just did Gemini completely using their own silicon and not having to pay the Nvidia or AMD tax.\n\nMicrosoft was really slow but now going to copy Google and start an effort to do their own TPUs.\n\nI think this is the future.  I do find it interesting because I am really old and I remember the day when it was like this in the past.  Where the IBMs and DECs and Suns all did their own chips instead of buying from third party.\n\nWe are going back there.", "score": 1, "replies": []}, {"body": "Especially on Linux where Nvidia is garbage!", "score": 1, "replies": []}]}
